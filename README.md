# research_ai
RAG implementation for research papers


## Installation
```bash
mamba env create -f environment.yml
conda activate research_ai
```


## Downloads / perquisites
* [Ollama](https://github.com/ollama/ollama) 
* [LM studio](https://lmstudio.ai/) 
* [Miniconda](https://docs.conda.io/en/latest/miniconda.html)
* GraphRAG: `pip install graphrag`


## Tutorial
https://youtu.be/BLyGDTNdad0?si=dBXt0zz8sdQSN0ff


## Intialize the project
#### Configuring GraphRAG Indexing
**Ref:** https://microsoft.github.io/graphrag/posts/config/init/ 

To start using GraphRAG, you need to configure the system. The init command is the easiest way to get started. It will create a .env and settings.yaml files in the specified directory with the necessary configuration settings. It will also output the default LLM prompts used by GraphRAG.
```bash
python -m graphrag.index [--init] [--root PATH]
```

#### Options
`--init` - Initialize the directory with the necessary configuration files.

`--root` PATH - The root directory to initialize. Default is the current directory.
#### Example
```bash
python -m graphrag.index --init --root ./ragtest
```

#### Output
The init command will create the following files in the specified directory:
settings.yaml - The configuration settings file. This file contains the configuration settings for GraphRAG.
.env - The environment variables file. These are referenced in the settings.yaml file.
prompts/ - The LLM prompts folder. This contains the default prompts used by GraphRAG, you can modify them or run the Auto Prompt Tuning command to generate new prompts adapted to your data.
#### Next Steps
After initializing your workspace, you can either run the Prompt Tuning command to adapt the prompts to your data or even start running the Indexing Pipeline to [index](#running-the-indexing-pipeline) your data. For more information on configuring GraphRAG, see the Configuration documentation



## Embedding endpoint
first, Download [LM studio](https://lmstudio.ai/), from the server tab download the embeddings model (`nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q8_0.gguf`) and start the server from GUI.

**Update the `settings.yaml` with the correct endpoint and models.**

**CHANGES**:

* encoding_model -> llm -> model: `gemma2`
* encoding_model -> llm -> api_base: `http://127.0.0.1:11434/v1` ## ollma server
* embeddings -> llm -> model: `nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q8_0.gguf`
* embeddings -> llm -> api_base: `http://127.0.0.1:1234/v1` ## embeddings server from LM studio


```
encoding_model: cl100k_base
skip_workflows: []
llm:
  api_key: ${GRAPHRAG_API_KEY}
  type: openai_chat # or azure_openai_chat
  # model: gpt-4-turbo-preview
  model: gemma2
  model_supports_json: true # recommended if this is available for your model.
  api_base: http://127.0.0.1:11434/v1
 

embeddings:
  ## parallelization: override the global parallelization settings for embeddings
  async_mode: threaded # or asyncio
  llm:
    api_key: ${GRAPHRAG_API_KEY}
    type: openai_embedding # or azure_openai_embedding
    model: nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q8_0.gguf #text-embedding-3-small 
    api_base: http://127.0.0.1:1234/v1 #https://<instance>.openai.azure.com

  ```



## Running the Indexing Pipeline
GraphRAG provides the ability to create domain adaptive templates for the generation of the knowledge graph. This step is optional, though is is highly encouraged to run it as it will yield better results when executing an Index Run.

The templates are generated by loading the inputs, splitting them into chunks (text units) and then running a series of LLM invocations and template substitutions to generate the final prompts. We suggest using the default values provided by the script, but in this page you'll find the detail of each in case you want to further explore and tweak the template generation algorithm.

#### Prerequisites
Before running the automatic template generation make sure you have already initialized your workspace with the graphrag.index --init command. This will create the necessary configuration files and the default prompts. Refer to the Init Documentation for more information about the initialization process.
Usage
You can run the main script from the command line with various options:
```bash
python -m graphrag.prompt_tune [--root ROOT] [--domain DOMAIN]  [--method METHOD] [--limit LIMIT] [--max-tokens MAX_TOKENS] [--chunk-size CHUNK_SIZE] [--no-entity-types] [--output OUTPUT]
```

#### Command-Line Options
`--root` (optional): The data project root directory, including the config files (YML, JSON, or .env). Defaults to the current directory.

`--domain` (optional): The domain related to your input data, such as 'space science', 'microbiology', or 'environmental news'. If left empty, the domain will be inferred from the input data.

`--method` (optional): The method to select documents. Options are all, random, or top. Default is random.

`--limit` (optional): The limit of text units to load when using random or top selection. Default is 15.
`--max-tokens` (optional): Maximum token count for prompt generation. Default is 2000.

`--chunk-size` (optional): The size in tokens to use for generating text units from input documents. Default is 200.
`--no-entity-types` (optional): Use untyped entity extraction generation. We recommend using this when your data covers a lot of topics or it is highly randomized.

`--output` (optional): The folder to save the generated prompts. Default is "prompts".
#### Example Usage
```
python -m graphrag.prompt_tune --root /path/to/project --domain "environmental news" --method random --limit 10 --max-tokens 2048 --chunk-size 256 --no-entity-types --output /path/to/output
```

or, with minimal configuration (suggested):

```
python -m graphrag.prompt_tune --root /path/to/project --no-entity-types
```


Document Selection Methods
The auto template feature ingests the input data and then divides it into text units the size of the chunk size parameter. After that, it uses one of the following selection methods to pick a sample to work with for template generation:
random: Select text units randomly. This is the default and recommended option.
top: Select the head n text units.
all: Use all text units for the generation. Use only with small datasets; this option is not usually recommended.

#### Modify Env Vars

After running auto-templating, you should modify the following environment variables (or config variables) to pick up the new prompts on your index run. Note: Please make sure to update the correct path to the generated prompts, in this example we are using the default "prompts" path.

`GRAPHRAG_ENTITY_EXTRACTION_PROMPT_FILE` = "prompts/entity_extraction.txt"
`GRAPHRAG_COMMUNITY_REPORT_PROMPT_FILE` = "prompts/community_report.txt"
`GRAPHRAG_SUMMARIZE_DESCRIPTIONS_PROMPT_FILE` = "prompts/summarize_descriptions.txt"